# PRIME: Plant cis-regulatory element DNA Language Model

[![Hugging Face](https://img.shields.io/badge/HuggingFace-Model-yellow)](https://huggingface.co/Zi-yangChen/PRIME_Single)
[![GitHub](https://img.shields.io/badge/GitHub-Repo-blue)](https://github.com/Zi-yangChen/PRIME)

PRIME is a **DNA language model** specifically pre-trained on **plant cis-regulatory elements (CREs)**.  
It captures sequence patterns and motif features, enabling applications in:
- Identification of cis-regulatory elements in plant genomes
- Motif feature extraction
- Sequence function prediction
- De novo design regulatory element design

> The model is also available on Hugging Face: [PRIME_Single](https://huggingface.co/Zi-yangChen/PRIME_Single) and [PRIME_BPE](https://huggingface.co/Zi-yangChen/PRIME_BPE)

---

## ğŸ“¦ Environment Setup

### 1. Clone this repository
```bash
git clone https://github.com/Zi-yangChen/PRIME.git
cd PRIME
```

### 2. Create a Conda environment
```bash
conda create -n prime python=3.9 -y
conda activate prime
pip install -r requirements.txt
```

## ğŸš€ Usage

### 1. Pre-training from scratch
You can use the `run_clm_noblock.py` script to pretrain a model using your own dataset.  
Before training the model, you need a `config.json` file, which you can download from either [PRIME_Single](https://huggingface.co/Zi-yangChen/PRIME_Single) or [PRIME_BPE](https://huggingface.co/Zi-yangChen/PRIME_BPE).

```bash
python ./run_clm_noblock.py \
    --model_type "gptneo" \
    --tokenizer_name "path/to/tokenizer" \
    --train_file "path/to/train_data" \
    --validation_file "path/to/val_data" \
    --learning_rate 3e-5 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --do_train \
    --do_eval \
    --output_dir "path/to/output" \
    --config_name "path/to/config" \
    --num_train_epochs 10 \
    --save_steps 10000 \
    --block_size 1024
```

In this script:  
1. `--model_type` â€” Specifies the model architecture; here we use GPT-Neo, an autoregressive language model.
2. `--tokenizer_name` â€” Path to the tokenizer (local directory or a Hugging Face repo ID).
3. `--train_file` â€” Path to the training dataset file.
4. `--validation_file` â€” Path to the validation dataset file.
5. `--learning_rate` â€” Initial learning rate.
6. `--per_device_train_batch_size` â€” Batch size per device for training.
7. `--per_device_eval_batch_size` â€” Batch size per device for evaluation.
8. `--do_train` â€” Enable the training process.
9. `--do_eval` â€” Enable evaluation during/after training.
10. `--output_dir` â€” Directory to save checkpoints, logs, and the final model.
11. `--config_name` â€” Path (or repo ID) to `config.json` containing the model configuration.
12. `--num_train_epochs` â€” Number of training epochs. *Alternatively, set `--max_steps` and adjust save/log/eval strategies.*
13. `--save_steps` â€” Interval (in steps) between checkpoint saves.
14. `--block_size` â€” Maximum sequence length (in tokens); longer sequences will be truncated.

### 2. Fine-tuning
å¦‚æœä½ æƒ³åŸºäºPRIMEé¢„è®­ç»ƒæ¨¡å‹å¹¶åˆ©ç”¨è‡ªå·±çš„æ•°æ®å¾®è°ƒï¼Œè¯·å‡†å¤‡å¥½tsvæ ¼å¼çš„æ•°æ®é›†æ–‡ä»¶ï¼Œç¡®ä¿æ–‡ä»¶ä¸­å«æœ‰sequenceåˆ—å’Œlabelåˆ—ã€‚
```bash
python finetune.py --config/config_for_prediction.json
```

### 3. De novo design
å¦‚æœä½ æƒ³åŸºäºPRIMEä»å¤´è®¾è®¡CREsï¼Œè¯·å‡†å¤‡å¥½txtæ ¼å¼çš„åºåˆ—æ–‡ä»¶
```bash
python generate.py --config/config_for_generation.json
```

### 4. Perturbation analysis
ä½ å¯ä»¥ä½¿ç”¨`perturb.py`è„šæœ¬è¿›è¡ŒåŸºäºæ‰°åŠ¨çš„å¯è§£é‡Šæ€§åˆ†æï¼Œå‘ç°CREsçš„å†…éƒ¨ç‰¹å¾ï¼
